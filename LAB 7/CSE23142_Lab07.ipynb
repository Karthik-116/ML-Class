{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzEqWgMs_3tD",
        "outputId": "8644a530-41ea-4cb2-a481-b1b4bce44faf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1: Loading and Preprocessing Data...\n",
            "Data Preprocessing Complete.\n",
            "\n",
            "Step 2: Defining Models and Hyperparameter Search Spaces...\n",
            "Rapidly tuning SVM...\n",
            "SVM tuning complete.\n",
            "Rapidly tuning Decision Tree...\n",
            "Decision Tree tuning complete.\n",
            "Rapidly tuning Random Forest...\n",
            "Random Forest tuning complete.\n",
            "Rapidly tuning AdaBoost...\n",
            "AdaBoost tuning complete.\n",
            "Rapidly tuning CatBoost...\n",
            "CatBoost tuning complete.\n",
            "Rapidly tuning XGBoost...\n",
            "XGBoost tuning complete.\n",
            "Rapidly tuning Gaussian NB...\n",
            "Gaussian NB tuning complete.\n",
            "Rapidly tuning MLP Classifier...\n",
            "MLP Classifier tuning complete.\n",
            "\n",
            "Step 3: Evaluating All Tuned Models...\n",
            "\n",
            "--- RAPID PERFORMANCE REPORT ---\n",
            "                        Accuracy  F1-score\n",
            "Model          Dataset                    \n",
            "SVM            Train       0.964     0.951\n",
            "               Test        0.544     0.340\n",
            "Decision Tree  Train       0.849     0.812\n",
            "               Test        0.468     0.305\n",
            "Random Forest  Train       1.000     1.000\n",
            "               Test        0.553     0.326\n",
            "AdaBoost       Train       0.539     0.253\n",
            "               Test        0.523     0.223\n",
            "CatBoost       Train       0.886     0.874\n",
            "               Test        0.556     0.345\n",
            "XGBoost        Train       1.000     1.000\n",
            "               Test        0.566     0.386\n",
            "Gaussian NB    Train       0.238     0.194\n",
            "               Test        0.231     0.196\n",
            "MLP Classifier Train       0.862     0.816\n",
            "               Test        0.547     0.423\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "\n",
        "from scipy.stats import randint, uniform # Utilities for defining hyperparameter search spaces.\n",
        "\n",
        "\n",
        "df = pd.read_csv(r'/content/final_min_features_filtered.csv') #DATA LOADING AND PREPROCESSING\n",
        "X = df.drop(columns=['file', 'run', 'label', 'onset_s'])\n",
        "y = df['label']\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.3, random_state=42, stratify=y_encoded)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "print(\"Data Preprocessing Complete.\\n\")\n",
        "\n",
        "models_to_tune = {}  # MODEL TRAINING AND HYPERPARAMETER TUNING\n",
        "# The parameter spaces remain the same, giving us the potential to find great parameters.\n",
        "models_to_tune['SVM'] = (SVC(probability=True, random_state=42),\n",
        "                         {'C': uniform(0.1, 10), 'gamma': uniform(0.001, 0.1)})\n",
        "models_to_tune['Decision Tree'] = (DecisionTreeClassifier(random_state=42),\n",
        "                                   {'max_depth': randint(5, 50)})\n",
        "models_to_tune['Random Forest'] = (RandomForestClassifier(random_state=42, n_jobs=-1),\n",
        "                                   {'n_estimators': randint(100, 200), 'max_depth': randint(10, 50)})\n",
        "models_to_tune['AdaBoost'] = (AdaBoostClassifier(random_state=42),\n",
        "                              {'n_estimators': randint(50, 200), 'learning_rate': uniform(0.01, 1.0)})\n",
        "models_to_tune['CatBoost'] = (CatBoostClassifier(random_state=42, silent=True),\n",
        "                              {'iterations': randint(100, 200), 'learning_rate': uniform(0.01, 0.3)})\n",
        "models_to_tune['XGBoost'] = (XGBClassifier(eval_metric='mlogloss', random_state=42, use_label_encoder=False),\n",
        "                             {'n_estimators': randint(100, 200), 'max_depth': randint(3, 10)})\n",
        "models_to_tune['Gaussian NB'] = (GaussianNB(),\n",
        "                                 {'var_smoothing': uniform(1e-10, 1e-7)})\n",
        "models_to_tune['MLP Classifier'] = (MLPClassifier(random_state=42, max_iter=500),\n",
        "                                    {'hidden_layer_sizes': [(50,), (100,)], 'alpha': uniform(0.0001, 0.01)})\n",
        "\n",
        "best_models = {}\n",
        "for name, (model, params) in models_to_tune.items():\n",
        "    print(f\"Rapidly tuning {name}...\")\n",
        "    random_search = RandomizedSearchCV(\n",
        "        model,\n",
        "        param_distributions=params,\n",
        "        n_iter=5,\n",
        "        cv=2,\n",
        "        random_state=42,\n",
        "        n_jobs=-1 # using all CPU cores for maximum parallelization.\n",
        "    )\n",
        "    random_search.fit(X_train_scaled, y_train)\n",
        "    best_models[name] = random_search.best_estimator_\n",
        "    print(f\"{name} tuning complete.\")\n",
        "\n",
        "\n",
        "print(\"\\nStep 3: Evaluating All Tuned Models...\")\n",
        "results = {}\n",
        "for name, model in best_models.items():\n",
        "    y_train_pred = model.predict(X_train_scaled)\n",
        "    y_test_pred = model.predict(X_test_scaled)\n",
        "    results[f\"{name}_Train\"] = {\n",
        "        'Accuracy': accuracy_score(y_train, y_train_pred),\n",
        "        'F1-score': f1_score(y_train, y_train_pred, average='macro')\n",
        "    }\n",
        "    results[f\"{name}_Test\"] = {\n",
        "        'Accuracy': accuracy_score(y_test, y_test_pred),\n",
        "        'F1-score': f1_score(y_test, y_test_pred, average='macro')\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "results_df = pd.DataFrame.from_dict(results, orient='index')\n",
        "results_df.index = pd.MultiIndex.from_tuples(\n",
        "    [(name.split('_')[0], name.split('_')[1]) for name in results_df.index],\n",
        "    names=['Model', 'Dataset']\n",
        ")\n",
        "print(\"\\n--- RAPID PERFORMANCE REPORT ---\")\n",
        "print(results_df.round(3))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kI6Z0sOBDxJT"
      },
      "execution_count": 5,
      "outputs": []
    }
  ]
}