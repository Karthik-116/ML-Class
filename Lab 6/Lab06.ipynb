{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23a743a5",
   "metadata": {},
   "source": [
    "# Lab06 – Decision Tree from Scratch\n",
    "### Using dataset: `example_unprocessed.xlsx`\n",
    "---\n",
    "This notebook implements all steps A1–A7:\n",
    "1. Entropy function\n",
    "2. Gini Index\n",
    "3. Information Gain & root node selection\n",
    "4. Binning (equal-width & frequency)\n",
    "5. Custom Decision Tree construction\n",
    "6. Tree visualization\n",
    "7. Decision boundary visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a782d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from typing import Any, List, Union\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_excel(\"example_unprocessed.xlsx\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c617c9e",
   "metadata": {},
   "source": [
    "## A1. Entropy Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03404771",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def entropy(labels: Union[pd.Series, List[Any], np.ndarray]) -> float:\n",
    "    counts = Counter([x for x in labels if pd.notna(x)])\n",
    "    total = sum(counts.values())\n",
    "    if total == 0: return 0.0\n",
    "    return -sum((c/total) * math.log2(c/total) for c in counts.values())\n",
    "\n",
    "# Example usage on target column\n",
    "TARGET_COLUMN = df.columns[-1]  # auto-select last column (adjust if needed)\n",
    "print(\"Entropy of target:\", entropy(df[TARGET_COLUMN]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb7a16e",
   "metadata": {},
   "source": [
    "## A2. Gini Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284a238b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gini_index(labels: Union[pd.Series, List[Any], np.ndarray]) -> float:\n",
    "    counts = Counter([x for x in labels if pd.notna(x)])\n",
    "    total = sum(counts.values())\n",
    "    if total == 0: return 0.0\n",
    "    return 1.0 - sum((c/total)**2 for c in counts.values())\n",
    "\n",
    "print(\"Gini Index of target:\", gini_index(df[TARGET_COLUMN]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba351e0d",
   "metadata": {},
   "source": [
    "## A3 & A4. Binning and Information Gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7beeb105",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def equal_width_binning(series: pd.Series, bins: int = 4):\n",
    "    binned, edges = pd.cut(series, bins=bins, retbins=True, labels=False, include_lowest=True, duplicates=\"drop\")\n",
    "    labels = [f\"bin_{int(i)}\" if pd.notna(i) else np.nan for i in binned]\n",
    "    return pd.Series(labels, index=series.index, dtype=\"category\"), list(edges)\n",
    "\n",
    "def frequency_binning(series: pd.Series, bins: int = 4):\n",
    "    binned, edges = pd.qcut(series, q=bins, retbins=True, labels=False, duplicates=\"drop\")\n",
    "    labels = [f\"bin_{int(i)}\" if pd.notna(i) else np.nan for i in binned]\n",
    "    return pd.Series(labels, index=series.index, dtype=\"category\"), list(edges)\n",
    "\n",
    "def bin_series(series: pd.Series, bins: int = 4, method: str = \"equal_width\"):\n",
    "    if method == \"equal_width\": return equal_width_binning(series, bins)\n",
    "    if method == \"frequency\": return frequency_binning(series, bins)\n",
    "    raise ValueError(\"Unknown method\")\n",
    "\n",
    "def information_gain(parent_labels: pd.Series, split_series: pd.Series) -> float:\n",
    "    parent_entropy = entropy(parent_labels)\n",
    "    total = len(split_series.dropna())\n",
    "    if total == 0: return 0.0\n",
    "    ig = parent_entropy\n",
    "    for v, idx in split_series.dropna().groupby(split_series).groups.items():\n",
    "        subset = parent_labels.loc[idx]\n",
    "        ig -= (len(idx) / total) * entropy(subset)\n",
    "    return ig\n",
    "\n",
    "# Example IG for first feature\n",
    "feat = df.columns[0]\n",
    "X_binned, _ = bin_series(df[feat], bins=4)\n",
    "print(\"Information Gain:\", information_gain(df[TARGET_COLUMN], X_binned))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57b4bd1",
   "metadata": {},
   "source": [
    "## A5. Custom Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d1aa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DTNode:\n",
    "    def __init__(self, is_leaf=False, prediction=None, feature=None, children=None):\n",
    "        self.is_leaf = is_leaf\n",
    "        self.prediction = prediction\n",
    "        self.feature = feature\n",
    "        self.children = children or {}\n",
    "\n",
    "def majority_class(labels: pd.Series):\n",
    "    cnt = Counter(labels.dropna())\n",
    "    return cnt.most_common(1)[0][0] if cnt else None\n",
    "\n",
    "def build_decision_tree(data: pd.DataFrame, target: str, features: List[str],\n",
    "                        bins=4, binning_method=\"equal_width\",\n",
    "                        max_depth=None, depth=0) -> DTNode:\n",
    "    y = data[target]\n",
    "    if len(y.dropna().unique()) <= 1:\n",
    "        return DTNode(is_leaf=True, prediction=majority_class(y))\n",
    "    if not features or (max_depth is not None and depth >= max_depth):\n",
    "        return DTNode(is_leaf=True, prediction=majority_class(y))\n",
    "\n",
    "    best_feature, best_ig, best_split = None, -1, None\n",
    "    for feat in features:\n",
    "        X = data[feat]\n",
    "        if pd.api.types.is_numeric_dtype(X):\n",
    "            split_series, _ = bin_series(X, bins=bins, method=binning_method)\n",
    "        else:\n",
    "            split_series = X.astype(\"category\")\n",
    "        ig = information_gain(y, split_series)\n",
    "        if ig > best_ig:\n",
    "            best_ig, best_feature, best_split = ig, feat, split_series\n",
    "\n",
    "    if not best_feature:\n",
    "        return DTNode(is_leaf=True, prediction=majority_class(y))\n",
    "\n",
    "    node = DTNode(is_leaf=False, feature=best_feature, children={})\n",
    "    remaining_features = [f for f in features if f != best_feature]\n",
    "\n",
    "    for val in best_split.dropna().unique():\n",
    "        idx = best_split[best_split == val].index\n",
    "        subset = data.loc[idx]\n",
    "        node.children[val] = build_decision_tree(\n",
    "            subset, target, remaining_features, bins, binning_method, max_depth, depth+1\n",
    "        )\n",
    "    return node\n",
    "\n",
    "def print_tree(node: DTNode, indent=\"\"):\n",
    "    if node.is_leaf:\n",
    "        print(indent + \"Leaf:\", node.prediction)\n",
    "    else:\n",
    "        print(indent + \"Feature:\", node.feature)\n",
    "        for val, child in node.children.items():\n",
    "            print(indent + f\" {val} ->\")\n",
    "            print_tree(child, indent + \"    \")\n",
    "\n",
    "features = [c for c in df.columns if c != TARGET_COLUMN]\n",
    "custom_tree = build_decision_tree(df.dropna(subset=[TARGET_COLUMN]), TARGET_COLUMN, features)\n",
    "print_tree(custom_tree)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c689f2",
   "metadata": {},
   "source": [
    "## A6. Decision Tree Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9bdc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Simple visualization with text output already shown above.\n",
    "# For a more graphical view, you could export to graphviz or matplotlib.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9b2af0",
   "metadata": {},
   "source": [
    "## A7. Decision Boundary Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf152329",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def decision_boundary_plot(data, target, features):\n",
    "    X = data[features].copy()\n",
    "    y = data[target].copy()\n",
    "    for col in X.columns:\n",
    "        if not pd.api.types.is_numeric_dtype(X[col]):\n",
    "            X[col] = X[col].astype(\"category\").cat.codes\n",
    "    if not pd.api.types.is_numeric_dtype(y):\n",
    "        y = y.astype(\"category\").cat.codes\n",
    "    clf = DecisionTreeClassifier(criterion=\"entropy\").fit(X.values, y.values)\n",
    "\n",
    "    x_min, x_max = X.values[:,0].min()-1, X.values[:,0].max()+1\n",
    "    y_min, y_max = X.values[:,1].min()-1, X.values[:,1].max()+1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 300),\n",
    "                         np.linspace(y_min, y_max, 300))\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3)\n",
    "    plt.scatter(X.values[:,0], X.values[:,1], c=y, edgecolors=\"k\")\n",
    "    plt.xlabel(features[0]); plt.ylabel(features[1])\n",
    "    plt.title(\"Decision Boundary\")\n",
    "    plt.show()\n",
    "\n",
    "if len(features) >= 2:\n",
    "    decision_boundary_plot(df.dropna(subset=[TARGET_COLUMN]), TARGET_COLUMN, features[:2])\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
